{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe Genes Project: Sequencing Pipeline\n",
    "\n",
    "Sean E. Guy<br/>\n",
    "Kryazhimskiy Lab<br/>\n",
    "Sep 2019\n",
    "\n",
    "---\n",
    "\n",
    "# 0. Introduction\n",
    "This document serves as a resource for the data analysis for the Safe Genes project completed before September 2019. It covers work done for the Asexual Evolution Experiment and the Mutation Accumulation Assay.  \n",
    "  \n",
    "The pipeline described for the Asexual Evolution Experiment is based on [McDonald, Rice, & Desai (2016)](https://doi.org/10.1038/nature17143) and was later adapted by JP Shaffer for use on the Triton Shared Computing Cluster (TSCC). This pipeline serves to identify and classify segregating variants in each population in order to compare the population-wide effects of CRISPR/Cas9 to Cas9 alone and wild-type.  \n",
    "  \n",
    "The pipeline for the Mutation Accumulation Assay was adapted from [Bloom, et al (2013)](https://doi.org/10.1038/nature11867). This pipeline identifies recombination events in each replicate cell line in an attempt to capture the genome-wide mutagenic effects of CRISPR/Cas9. This pipeline is largely incomplete and will require additional scripting and debugging to achieve more reliable results.  \n",
    "  \n",
    "In their current state, the pipelines require a lot of input on the user's end. Thus, the user should take care when formatting tables, naming files, organizing directories, and installing programs. This manual will attempt to provide a general guideline for these steps. Should the user require additional assistance, they may try [contacting the author](mailto:sguy@ucsd.edu); results may vary.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Table of Contents\n",
    "|Number|Section|\n",
    "|:--|:--:|\n",
    "|0| Introduction |\n",
    "|1| Table of Contents|\n",
    "|2| Program, Version Requirements|\n",
    "|3| Visual Summary|\n",
    "|4| Script Documenation|\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Set Up\n",
    "\n",
    "Suggestions on how to set up your software and directories prior to attempting to run the sequencing pipelines.  \n",
    "  \n",
    "## Recommend File Transfer and Editing Software\n",
    "Before starting, install software to assist file exchanges among the lab's shared Google Drive, your personal computer, and the TSCC. A combination of [FileZilla](https://filezilla-project.org/) and [Google Drive File Stream](https://www.google.com/drive/download/) will work, but this method taxes the the user's personal computer and internet connection. Each file is downloaded from Google Drive by File Stream to local memory, uploaded to TSCC by FileZilla, then automatically wiped from local memory by File Stream. A free solution that permits transfer by SFTP directly from Drive to TSCC would be desirable.  \n",
    "  \n",
    "While editing and troubleshooting your software, having an active terminal immediately available for testing the code is ideal. Visual Studio Code is included with the standard Anaconda distribution and will already have most required packages installed. VS Code can open a terminal in your sidebar or below your code so you won't have to switch between tabs or windows to access the TSCC.\n",
    "\n",
    "## Directory Structure\n",
    "The process of variant calling is designed to work in parallel using the TSCC. Maintaining a clear directory structure will help reduce the incidence of bugs and errors. Below is a rough description of the directory structure originally used for data analysis.  \n",
    "  \n",
    "Each user in the TSCC is assigned a home directory and a scratch directory. `/home/user` has limited memory, so only store necessary files here that will be required in the long-term. Such files should also be backed up in the \"SKLAB DATA\" shared drive. `/oasis/tscc/scratch/user/` should be the primary working directory as it has more memory. However, the TSCC will periodically wipe this memory in order to accomodate its users, so always back up important files.\n",
    "\n",
    "The scratch directory contains separate directories for data, software, custom code, and logs. In \"data\", create separate directories for the raw sequencing files and one directory for reference genomes. For the Experimental Evolution Assay, strains were analysed independently, so these were assigned independent directories in \"data\", each with their own sub-directories for fastq, BAM, and VCF files generated during variant calling. \"Software\" contains all of the packages we download and use (e.g. Trimmomatic, Bowtie2, GATK). \"Code\" has all of the custom scripts described in this manual. \"Log\" stores the input, output, and error messages from each of the jobs submitted to the cluster.\n",
    "\n",
    "## Installation\n",
    "SFTP the custom scripts from Google Drive into the \"code\" directory on the cluster. Before running the scripts, provide permission to run and edit the scripts from command line: `chmod 777 /oasis/tscc/scratch/user/code/*` or `chmod ug=rwx`. Individual scripts can be separately transferred and overwritten as needed.  \n",
    "  \n",
    "Download software from their respective sources (see below). Transfer to the \"software\" directory and provide these permission to run as well. Follow the installation instructions provided in each link. While installing, it would be wise to run the process as an interactive job on TSCC to reduce load on the login server: `qsub -I -l walltime=1:00:00`. Once installation is complete, `exit` will end the job.  \n",
    "  \n",
    "* [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)\n",
    "* [Bowtie2](https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.3.5.1/)\n",
    "* [GATK 2.6](https://software.broadinstitute.org/gatk/download/auth?package=GATK-archive&version=2.6-5-gba531bd)\n",
    "* [GATK 4.1](https://software.broadinstitute.org/gatk/download/)\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Visual Summary\n",
    "\n",
    "(Make some tables or flowcharts like the ones in GATK that show the order in which you can implement the modules for different analyses)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test image  \n",
    "  \n",
    "![IMAGE](https://raw.githubusercontent.com/SeanEGuy/sklab_safe_genes/master/het_counts.png)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Script Documentation\n",
    "\n",
    "Below are full descriptions and examples of each key step in the sequencing pipelines. In the actual file names, the version number(s) will generally be just left of the file extension, separated from the rest of the file name with a \".\" or \"_\". Check the script documentation at the top of each python script for the exact formatting of the input.\n",
    "\n",
    "**rename_fastq.py & rename_fastq.sh**\n",
    "  * Description:  \n",
    "    Renames Illumina sequencing files using a table of strain and replicate numbers. The resulting files should be named following the pattern, \"[STRAIN]\\_[REPLICATE]-[GENERATION].fastq\" (e.g. \"27_1-100\" referring to strain 27, replicate population 1, generation 100).\n",
    "  * Newest versions: rename_fastq_053119.py and rename_MA_041519.sh\n",
    "  * Input:\n",
    "      * plans, (PATH) directory with plain-text documents containing sample, replicate, and sequencing lane\n",
    "      * data, (PATH) directory with raw Illumina sequencing files\n",
    "      * fastq, (PATH) directory to which renamed files will be output\n",
    "  * Return: None\n",
    "  * Usage Notes:\n",
    "      * Using the sequencing manifests as a guide, generate a \"planfile\" that will direct the python script to match each index to the correct strain and sample. Only one plan file should be used per sequencing batch. Keep the renamed files separate from the originals and in unique directories for each sequencing batch.\n",
    "      * Illumina may use different naming patterns depending on the information submitted when ordering the sequences. Locate the section in the code that searches for unique patterns in each file name (line 129 in version 053119) and adjust it according to the pattern in your file names.\n",
    "      * Example:  \n",
    "        The script below looks for the pattern `[Sample Number]_S[Sample Number]` for each row in the sample information table provided. This is accomplished by concatenating `num`, `'_S'`, then `num`.\n",
    "      * Compatible with both compressed and uncompressed files.\n",
    "      * To prepare for submission to TSCC, change `#PBS` settings for notifications, input, output, and error messages (see [the TSCC User Guide](https://www.sdsc.edu/support/user_guides/tscc.html) for details). Then submit by putting `$ qsub ./rename_fastq.sh` into the command line.\n",
    "      * Note: One should generally use the absolute path instead of relative paths with `./` or `../`.\n",
    "  * Example \"planfile\" for the April 2018 sequencing run, bash submission script, and python script:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lib\tstrain\tnum\tpop\tgen\n",
    "N707\t27\tS501\t0\t0\n",
    "N707\t27\tS504\t1\t100\n",
    "N707\t27\tS505\t2\t100\n",
    "N707\t27\tS506\t3\t100\n",
    "N708\t27\tS505\t1\t200\n",
    "N708\t27\tS506\t2\t200\n",
    "N708\t27\tS507\t3\t200\n",
    "N709\t27\tS506\t1\t300\n",
    "N709\t27\tS507\t2\t300\n",
    "N709\t27\tS508\t3\t300\n",
    "N710\t27\tS507\t1\t400\n",
    "N710\t27\tS508\t2\t400\n",
    "N711\t27\tS501\t3\t400\n",
    "N707\t28\tS502\t0\t0\n",
    "N707\t28\tS507\t1\t100\n",
    "N707\t28\tS508\t2\t100\n",
    "N708\t28\tS501\t3\t100\n",
    "N708\t28\tS508\t1\t200\n",
    "N709\t28\tS501\t2\t200\n",
    "N709\t28\tS502\t3\t200\n",
    "N710\t28\tS501\t1\t300\n",
    "N710\t28\tS502\t2\t300\n",
    "N710\t28\tS503\t3\t300\n",
    "N711\t28\tS502\t1\t400\n",
    "N711\t28\tS503\t2\t400\n",
    "N711\t28\tS504\t3\t400\n",
    "N711\t28\tS508\t1\t500\n",
    "N707\t29\tS503\t0\t0\n",
    "N708\t29\tS502\t1\t100\n",
    "N708\t29\tS503\t2\t100\n",
    "N708\t29\tS504\t3\t100\n",
    "N709\t29\tS503\t1\t200\n",
    "N709\t29\tS504\t2\t200\n",
    "N709\t29\tS505\t3\t200\n",
    "N710\t29\tS504\t1\t300\n",
    "N710\t29\tS505\t2\t300\n",
    "N710\t29\tS506\t3\t300\n",
    "N711\t29\tS505\t1\t400\n",
    "N711\t29\tS506\t2\t400\n",
    "N711\t29\tS507\t3\t400\n",
    "N712\t29\tS501\t1\t500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l walltime=1:00:00\n",
    "#PBS -M amartsul@ucsd.edu\n",
    "#PBS -m abe\n",
    "#PBS -N rename_MA_march2019\n",
    "#PBS -o /oasis/tscc/scratch/amartsul/Safe_genes/Log/rename_MA_march2019.out\n",
    "#PBS -e /oasis/tscc/scratch/amartsul/Safe_genes/Log/rename_MA_march2019.err\n",
    "\n",
    "# Plan file path\n",
    "PLANS=/oasis/tscc/scratch/amartsul/Safe_genes/code/src/ma_prelim_planfile.txt \n",
    "# Where the decompressed fastq files come from\n",
    "SOURCE=/oasis/tscc/scratch/sguy/SafeGenes/data/mut_acc_prelim_mar19\n",
    "# Destination directory for renamed fastq files\n",
    "FQDIR='/oasis/tscc/scratch/amartsul/Safe_genes/data/MA_saples/Fasta'\n",
    "# Location of python script for renaming\n",
    "PYSCRIPT=/oasis/tscc/scratch/amartsul/Safe_genes/code/src/rename_fastq_050319.py\n",
    "\n",
    "module load python\n",
    "\n",
    "python ${PYSCRIPT} plan=${PLANS} data=${SOURCE} fastq=${FQDIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Summary:\n",
    "Renames Illumina read files according to sample and generation and\n",
    "copies to the strain-specific directory.\n",
    "\n",
    "Usage:\n",
    "Designed to be called from shell, as shown below. The order of inputs\n",
    "is not important, but each must lead with \\'plan=\\', \\'data=\\', or\n",
    "\\'fastq=\\' to be recognized. A value must be provided in each of these\n",
    "three fields. DO NOT include the final foward slash, \\'/\\' in directory\n",
    "paths. DO include the entire path down to the root.\n",
    "\n",
    "Before using, the specific globbing patterns for identifying the source\n",
    "files must be changed according to the naming scheme of your planfiles\n",
    "and Illumina files. Also make sure that only planfiles are present in\n",
    "the plan directory provided.\n",
    "\n",
    "> python [PATH]/rename_fastq.py \\\\\n",
    "    plan=[PLANDIR] \\\\\n",
    "    data=[DATADIR] \\\\\n",
    "    fastq=[FQDIR]\n",
    "\n",
    "Update Notes:\n",
    "3/21/19 It's OK to use on compressed files now.\n",
    "        Adding new column for strain so all samples processed together.\n",
    "        For destination (fastq), replace strain number with ##; will\n",
    "        generate destination files using the planfile.\n",
    "\n",
    ":@param plans: (str) Directory containing formatted sample data sheets\n",
    ":@param data: (str) Directory containing raw, compressed read files\n",
    ":@param fastq: (str) Directory to place decompressed & renamed files\n",
    "'''\n",
    "\n",
    "#Get modules for interacting w/ environment\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "#Make dictionary for paths and their purpose\n",
    "paths = {'plan': '', 'data': '', 'fastq': ''}\n",
    "\n",
    "#Pull values from input\n",
    "for flag in sys.argv: #Loop through bash inputs\n",
    "    if flag[:5] == 'plan=': #Checking flag\n",
    "        paths['plan'] = flag[5:] #Set plan file directory path\n",
    "    elif flag[:5] == 'data=': #Checking flag\n",
    "        paths['data'] = flag[5:] #Set data directory path\n",
    "    elif flag[:6] == 'fastq=': #Checking flag\n",
    "        paths['fastq'] = flag[6:] #Set data directory path\n",
    "    elif flag[-15:] == 'rename_fastq.py':\n",
    "        pass #Ignore path to this script\n",
    "    else:\n",
    "        print \"{0} not recognized as input\".format(flag)\n",
    "\n",
    "# Assertions need to be updated to accomodate multi-dir input\n",
    "'''\n",
    "#Required variables should be set\n",
    "for directory in paths:\n",
    "    #Checking non-zero length so it exists\n",
    "    assert( len(paths[directory]) > 0 ), \\\n",
    "        \"No value set for {0} directory\".format(directory)\n",
    "    #Path variables should lead to an extant dir\n",
    "    assert( os.path.exists(paths[directory]) ), \\\n",
    "        \"Path {0} not found\".format(paths[directory])\n",
    "    #Check that path goes to a directory, not a file\n",
    "    assert( os.path.isdir(paths[directory]) ), \\\n",
    "        \"{0} is not a directory\".format(paths[directory])\n",
    "'''\n",
    "\n",
    "#Placeholders for sample data, files names\n",
    "lib = ''\n",
    "num = ''\n",
    "pop = ''\n",
    "gen = ''\n",
    "src1, src2 = ['', '']\n",
    "dest1, dest2 = ['', '']\n",
    "\n",
    "#Pulling up the source fastq file names for quick access\n",
    "sources = os.listdir( paths['data'] )\n",
    "\n",
    "def append_by_chunks( source, destination, chunk_size = 10**6 ):\n",
    "    '''\n",
    "    Appends files if multiple sequencing files exist for the same\n",
    "    strain/replicate/generation.\n",
    "\n",
    "    :param source:      raw seqeuncing file\n",
    "    :param destination: file onto which source is appended\n",
    "    :param chunk_size:  (int) memory size to split source up by\n",
    "    '''\n",
    "    # Access destination in append mode\n",
    "    with open( destination, 'a') as data_dest:\n",
    "        # Access source in read mode\n",
    "        with open( source, 'r' ) as data_src:\n",
    "            # Create the buffer to store each chunk\n",
    "            data_chunk = data_src.read( chunk_size )\n",
    "            # Loop through chunks until empty\n",
    "            while bool(data_chunk):\n",
    "                # write each chunk to destination\n",
    "                data_dest.write( data_chunk )\n",
    "                # get the next chunk of data\n",
    "                data_chunk = data_src.read( chunk_size )\n",
    "    # No output needed; just the file manipulation\n",
    "    return None\n",
    "\n",
    "with open( paths['plan'], 'r') as sample_table:\n",
    "    #Looping through rows in each planfile\n",
    "    for sample_row in sample_table:\n",
    "        # Testing for an empty row\n",
    "        if sample_row.split() == []:\n",
    "            # Skip empty rows\n",
    "            continue\n",
    "        # If the row has some text\n",
    "        else:\n",
    "            # Make sure there are 5 headers in the table\n",
    "            assert( len(sample_row.split(\"\\t\")) == 5 ), \\\n",
    "                \"Required headers in planfile: lib, strain, num, pop, gen\"\n",
    "            #Separating individual items in sample data\n",
    "            lib, strain, num, pop, gen = sample_row.split()\n",
    "        #Skipping the header line\n",
    "        if lib == 'lib':\n",
    "            continue\n",
    "        else:\n",
    "            #Pulling the source name out of the sample data\n",
    "            for file_name in source:\n",
    "                # Check for unique identifier(s)\n",
    "                unique_match = bool(\n",
    "                    # Check the Gene Drive library & sample number\n",
    "                    num + \"_S\" + num in file_name\n",
    "                )\n",
    "                #Matching to the unique portion of the file name + R1\n",
    "                if unique_match and '_R1_' in file_name:\n",
    "                    #Save the R1 match file path\n",
    "                    src1 = '{0}/{1}'.format( paths['data'], file_name )\n",
    "                    # QC\n",
    "                    print src1\n",
    "                #Matching to the unique portion of the file name + R2\n",
    "                elif unique_match and '_R2_' in file_name:\n",
    "                    #Save the R2 match file path\n",
    "                    src2 = '{0}/{1}'.format( paths['data'], file_name )\n",
    "                    # QC\n",
    "                    print src2\n",
    "                else:\n",
    "                    pass\n",
    "        #Check that source files were located\n",
    "        for src in [src1, src2]:\n",
    "            assert( src != '' ), \\\n",
    "                \"Source file of library {0}, sample {1} for {2}-{3}_{4} not found\".format(\n",
    "                    lib,\n",
    "                    num,\n",
    "                    strain,\n",
    "                    pop,\n",
    "                    gen\n",
    "                )\n",
    "        #Generate destination file names; replace ## with strain\n",
    "        dest1 = '{0}/{1}_{2}-{3}_R1.fastq.gz'.format(\n",
    "            paths['fastq'].replace('##', strain),\n",
    "            strain,\n",
    "            pop,\n",
    "            gen\n",
    "        )\n",
    "        dest2 = '{0}/{1}_{2}-{3}_R2.fastq.gz'.format(\n",
    "            paths['fastq'].replace('##', strain),\n",
    "            strain,\n",
    "            pop,\n",
    "            gen\n",
    "        )\n",
    "        # See if the destination exists\n",
    "        if os.path.isfile(dest1):\n",
    "            # Append source 1 to its destination\n",
    "            append_by_chunks( src1, dest1 )\n",
    "            # Append source 2 to its destination\n",
    "            append_by_chunks( src2, dest2 )\n",
    "        else:\n",
    "            #Copy files to the destination if new\n",
    "            shutil.copy( src1, dest1 )\n",
    "            shutil.copy( src2, dest2 )\n",
    "        #Reset the paths for error detection\n",
    "        src1, src2, dest1, dest2 = ['', '', '', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**combine_raw_reads.sh & combine_raw_reads.py**\n",
    "  * Description:  \n",
    "    Some samples from the asexual evolution experiment were resequenced in order to improve coverage. This script combines raw FASTQ files from different directories by file name such that all reads belonging to each sample are processed together.\n",
    "  * Newest version: combine_raw_reads_032919.sh & combine_raw_reads.py\n",
    "  * Input:\n",
    "      * source1, (PATH) where one set of fastq files are stored\n",
    "      * source2, (PATH) where a matching set of fastq files are stored\n",
    "      * destination, (PATH) where to output the newly combined fastq files\n",
    "  * Usage Notes:\n",
    "      * The bash script handles the python script. This pattern of a bash script running another script is consistent for most scripts submitted to run on the TSCC.\n",
    "      * Before running, check that the paths in the bash script lead to the desired source and output directories.\n",
    "      * It is highly recommended that your destiation directory be different from either of your source directories. An infite loop may occur otherwise.\n",
    "      * To run the script in TSCC, first adjust the `#PBS` params for job time, email alerts, etc (see [the TSCC User Guide](https://www.sdsc.edu/support/user_guides/tscc.html) for details). Then submit the bash script from command line using `$ qsub ./combine_raw_reads.sh` in the TSCC.\n",
    "  * Example bash submission and python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l walltime=5:00:00\n",
    "#PBS -M sguy@ucsd.edu\n",
    "#PBS -m abe\n",
    "#PBS -N combine_27\n",
    "#PBS -o /oasis/tscc/scratch/sguy/SafeGenes/log/combine_27_032919.out\n",
    "#PBS -e /oasis/tscc/scratch/sguy/SafeGenes/log/combine_27_032919.err\n",
    "\n",
    "# Python script that merges reads from two separate runs\n",
    "SCRIPT=/oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/combine_raw_reads.py\n",
    "\n",
    "# Where to find the reads named after STRAIN_REPLICATE-GENERATION_[R1|R2].fastq.gz\n",
    "SOURCE1=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC27/fastq_SEP18\n",
    "SOURCE2=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC27/fastq_JAN19\n",
    "# Where to send the newly combined reads\n",
    "DEST=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC27/fastq_combined\n",
    "\n",
    "# Call the program and provide the parameters\n",
    "python ${SCRIPT} source1=${SOURCE1} source2=${SOURCE2} destination=${DEST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For combining raw sequencing files from two separate runs. This should\n",
    "actually be agnostic to most file contents, but this is how it fits in\n",
    "the sequencing and mutation detection pipeline. The DNA sequences must\n",
    "be in the same format and in separate directories with no other files.\n",
    "\n",
    "USAGE\n",
    "Call this program with a bash script. Always use absolute paths. Do not\n",
    "include the last / of a directory path.\n",
    "\n",
    "User$ python ~/combine_raw_reads.py \\\\\n",
    "    source1=[INPUT] \\\\\n",
    "    source2=[INPUT] \\\\\n",
    "    destination=[OUTPUT]\n",
    "\n",
    ":param source1:      Directory of raw sequencing files\n",
    ":param source2:      Directory of raw sequencing files\n",
    ":param destination:  Directory to send the newly combined files\n",
    "'''\n",
    "# Get modules for file manipulation\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Initialize source/destination vars\n",
    "source1 = ''\n",
    "source2 = ''\n",
    "destination = ''\n",
    "\n",
    "# Getting inputs from bash call\n",
    "for command in sys.argv:\n",
    "    # Assign directory to source 1 if flag matches\n",
    "    if command[:8] == 'source1=':\n",
    "        source1 = command[8:]\n",
    "    # Assign directory to source 2 if flag matches\n",
    "    elif command[:8] == 'source2=':\n",
    "        source2 = command[8:]\n",
    "    # Assign directory to destination if flag matches\n",
    "    elif command[:12] == 'destination=':\n",
    "        destination = command[12:]\n",
    "    # Ignore other inputs\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Make sure each directory is real\n",
    "for given_dir in [source1, source2, destination]:\n",
    "    assert os.path.isdir(given_dir), \\\n",
    "        given_dir + \"is not a directory\"\n",
    "\n",
    "# List all of the files in each source\n",
    "source_list1 = os.listdir(source1)\n",
    "source_list2 = os.listdir(source2)\n",
    "\n",
    "# Libraries to track progress; True = copied; False = not copied\n",
    "sources_copied1, sources_copied2 = {}, {}\n",
    "# Fill the dict with each file from each source 1 as False\n",
    "for one_source in source_list1:\n",
    "    sources_copied1[one_source] = False\n",
    "# Fill the dict with each file from each source 2 as False\n",
    "for two_source in source_list2:\n",
    "    sources_copied2[two_source] = False\n",
    "\n",
    "# Loop through each file name in source 1\n",
    "for fname1 in source_list1:\n",
    "    # Start by sending the file over\n",
    "    shutil.copy(source1 + '/' + fname1, destination)\n",
    "    # Mark as copied in source 1's dict\n",
    "    sources_copied1[fname1] = True\n",
    "    # Find match in other directory\n",
    "    if fname1 in source_list2:\n",
    "        # Open source 2's matching file into the destination\n",
    "        with open(source2 + '/' + fname1, 'r') as copied_from:\n",
    "            # Open the newly made file from source 1\n",
    "            with open(destination + '/' + fname1, 'a') as copied_to:\n",
    "                # Designate a manageable buffer size\n",
    "                b_size = 10**6\n",
    "                # Create a buffer variable\n",
    "                data_buffer = copied_from.read(b_size)\n",
    "                # Loop thru chunks of data; ends when none left to copy\n",
    "                while data_buffer:\n",
    "                    # Append the buffered data to the destination\n",
    "                    copied_to.write(data_buffer)\n",
    "                    # Get next chunk of data\n",
    "                    data_buffer = copied_from.read(b_size)\n",
    "        # Mark as copied in source 2's dict\n",
    "        sources_copied2[fname1] = True\n",
    "    # Just move on if there's no match\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Going back to source 2\n",
    "for fname2 in source_list2:\n",
    "    # Skip if it's been copied already (== True)\n",
    "    if sources_copied2[fname2]:\n",
    "        pass\n",
    "    # Copy it over if it hasn't been copied already\n",
    "    else:\n",
    "        shutil.copy(source2 + '/' + fname2, destination)\n",
    "        sources_copied2[fname2] = True\n",
    "\n",
    "# Make sure everything has been copied\n",
    "for check1 in source_list1:\n",
    "    assert(sources_copied1[check1]), \\\n",
    "        source1 + '/' + check1 + \" not copied\"\n",
    "for check2 in source_list2:\n",
    "    assert(sources_copied2[check2]), \\\n",
    "        source2 + '/' + check2 + \" not copied\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**trimmomatic_fastq.sh & qsub_trimmomatic.sh**\n",
    "* Description:  \n",
    "  Trim and filter reads prior to alignment. Removes Illumina adapter sequences low-quality ends.\n",
    "* Newest Version (for asexual evolution): 040119\n",
    "* Input:\n",
    "  * FQDIR, (PATH) location of renamed fastq files\n",
    "  * OUTDIR, (PATH) location to save trimmed fastq files\n",
    "  * TRIMMO, (PATH) location to the Trimmomatic software\n",
    "  * SCRIPT, (PATH) bash script that runs the trimmomatic script\n",
    "  * ADAPTER, (PATH) Illumina Nextera adapter sequence in fasta format\n",
    "  * LOG, (PATH) where to save error/output text files from TSCC\n",
    "* Output:  \n",
    "  Generates four trimmed files, two fastq files of paired reads and two of unpairedreads, from each pair of raw sequencing files.\n",
    "* Usage Notes:\n",
    "  * The example script below is written for Trimmomatic 0.38. Check the [Trimmomatic website](http://www.usadellab.org/cms/?page=trimmomatic) for updated manuals regarding flags and formatting. Trimmomatic version 0.35 is also available as a module in TSCC: `module load trimmomatic` (Note that the archived version currently available online is 0.36, so formatting may be different).\n",
    "  * The submission script only looks for files with \"R1.fastq\" in the name. Ensure that all sequencing files are in their own, separate directory and that their file names are formatted correctly.\n",
    "  * In the submission script, change the dates on the error (`-e`) and output (`-o`) file names in the qsub command for your own tracking purposes.\n",
    "  * In the main script, change or remove the email command (`#PBS -M` and `#PBS -m`) for updates on the job's progress. If doing multiple jobs, you can move this set of commands to the submission script as additional flags in qsub.\n",
    "  * The main script switches from the TSCC's default Java to Java 1.8 in order to run a more current version of Trimmomatic. The location of this version of Java should be consistent for any user, so contact TSCC admin if the package appears to no longer be available.\n",
    "* Submission and main bash scripts from 04/01/2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Submission script for trimming all .fastq.gz's in a strain directory\n",
    "\n",
    "# Directory containing the sorted, renamed reads\n",
    "export FQDIR=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC27/fastq_combined\n",
    "# Directory to store the trimmed reads\n",
    "export OUTDIR=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC27/fastq_trimmed\n",
    "# Location of the trimmomatic jar file\n",
    "export TRIMMO=/oasis/tscc/scratch/sguy/SafeGenes/software/Trimmomatic-0.38/trimmomatic-0.38.jar\n",
    "# Location of the script that runs trimmomatic using the plan files\n",
    "SCRIPT=/oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/trimmomatic_fastq_040119.sh\n",
    "# Fasta of the Illumina adapter sequence to remove\n",
    "export ADAPTER=/oasis/tscc/scratch/sguy/SafeGenes/software/Trimmomatic-0.38/adapters/NexteraPE-PE.fa\n",
    "# Directory to print output/errors\n",
    "LOG=/oasis/tscc/scratch/sguy/SafeGenes/log\n",
    "\n",
    "# Index to uniquely name log files.\n",
    "i=0\n",
    "\n",
    "for r1file in $FQDIR/*R1.fastq; do\n",
    "    i=$(($i+1))\n",
    "    export R1FILE=$r1file\n",
    "    export R2FILE=${R1FILE/R1/R2}\n",
    "    qsub \\\n",
    "        -V \\\n",
    "        -o ${LOG}/trim_27_${i}_040119.out \\\n",
    "        -e ${LOG}/trim_27_${i}_040119.err \\\n",
    "        -N trim_27-${i} \\\n",
    "        ${SCRIPT}\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l nodes=1,walltime=2:00:00\n",
    "#PBS -M sguy@ucsd.edu\n",
    "#PBS -m abe\n",
    "\n",
    "# Switching to java 1.8\n",
    "export PATH=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/bin:$PATH\n",
    "\n",
    "# Record inputs to log\n",
    "echo fastq inputs:; echo $R1FILE; echo $R2FILE\n",
    "echo Illumina adapter: $ADAPTER\n",
    "echo Trimmomatic location: $TRIMMO\n",
    "\n",
    "# Generate the output file paths leading to the output directory\n",
    "NEW_R1=${R1FILE/$FQDIR/$OUTDIR}\n",
    "NEW_R2=${R2FILE/$FQDIR/$OUTDIR}\n",
    "# Send Trimmomatic's logs to same folder; changing file ext later\n",
    "TRIMDIR=${R1FILE/$FQDIR/$OUTDIR}\n",
    "\n",
    "java -jar ${TRIMMO} PE \\\n",
    "    -trimlog ${TRIMDIR/R1.fastq/trimlog.txt} \\\n",
    "    $R1FILE $R2FILE \\\n",
    "    ${NEW_R1/R1.fastq/R1P.trimmed.fastq} ${NEW_R1/R1.fastq/R1U.trimmed.fastq} \\\n",
    "    ${NEW_R2/R2.fastq/R2P.trimmed.fastq} ${NEW_R2/R2.fastq/R2U.trimmed.fastq} \\\n",
    "    ILLUMINACLIP:${ADAPTER}:1:30:10 \\\n",
    "    LEADING:3 \\\n",
    "    TRAILING:3 \\\n",
    "    SLIDINGWINDOW:10:7 \\\n",
    "    HEADCROP:10 \\\n",
    "    CROP:80 \\\n",
    "    MINLEN:20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bowtie_samtools.sbatch and submit_bowtie.sh**\n",
    "  * Description:  \n",
    "    Aligns each FASTQ file to a reference genome, generating BAM files.\n",
    "  * Newest version: 040119\n",
    "  * Input:\n",
    "      * FQDIR, (PATH) directory containing all FASTQ files\n",
    "      * BAMDIR, (PATH) directory to output BAM alignment files\n",
    "      * REFPREFIX, (PATH) path to genome files excluding file extension\n",
    "      * LOGDIR, (PATH) where to save TSCC output/error messages\n",
    "      * DATADIR, (PATH) parent directory of FQDIR and BAMDIR\n",
    "  * Output:  \n",
    "    Prints \"Submitting [FILE NAME]\" to console for each file submitted.\n",
    "  * Usage Notes:\n",
    "      * Update paths in submit_bowtie.sh before submitting job to TSCC\n",
    "      * A 2 hour run time is generous. Each job should finish in well under half an hour.\n",
    "      * Only FQDIR should only contain FASTQ files that you want aligned\n",
    "      * `name_only=\"$(cut -d'/' -f10 <<<${R1FILE})\"` pulls out the FASTQ file name. The `-f10` option selects the 10th item in the file path separated by forward slashes. Change this number according to the depth of your file directory.\n",
    "      * When ready, run the submission script directly from command line: `$ ./submit_bowtie.sh`.The most recent version submits one job per R1 paired sequencing file. Do not submit more than 2000 jobs at once.\n",
    "      * This version does not delete the \"unsorted\" BAM files after sorting indexes. After you QC the alignments, feel free to delete all files that end with \".unsorted.bam\".\n",
    "      * This version submits both the paired and unpaired read ends for alignment. Feel free to remove the `-U` flag from the Bowtie command such that only paired reads are used.\n",
    "  * Submission and main bowtie_samtools.sbatch scripts from 04/01/19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Submit bowtie jobs that will align trimmed reads to a reference genome\n",
    "# Chenged so it doesn't need a plan file -SEG 04/01/19\n",
    "\n",
    "export REFPREFIX=/oasis/tscc/scratch/sguy/SafeGenes/data/reference_genomes/Sc_W303_v2_yKC27_genomic\n",
    "LOGDIR=/oasis/tscc/scratch/sguy/SafeGenes/log\n",
    "DATADIR=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC27\n",
    "\n",
    "export FQDIR=${DATADIR}/fastq_trimmed\n",
    "export BAMDIR=${DATADIR}/bam\n",
    "\n",
    "for R1PFILE in ${FQDIR}/*R1P.trimmed.fastq; do\n",
    "    export R1FILE=${R1PFILE}\n",
    "    export R2FILE=${R1PFILE/R1P/R2P}\n",
    "    name_only=\"$(cut -d'/' -f10 <<<${R1FILE})\"\n",
    "    export INDEX=${name_only/_R1P.trimmed.fastq/}\n",
    "    echo \"Submitting ${INDEX}\"\n",
    "    qsub \\\n",
    "        -V \\\n",
    "        -N align_${INDEX} \\\n",
    "        -o ${LOGDIR}/align_${INDEX}_040219.out \\\n",
    "        -e ${LOGDIR}/align_${INDEX}_040219.err \\\n",
    "        /oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/bowtie_samtools_040119.sbatch\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l nodes=1,walltime=02:00:00\n",
    "#PBS -M sguy@ucsd.edu\n",
    "#PBS -m abe\n",
    "\n",
    "module load bowtie2\n",
    "module load samtools\n",
    "\n",
    "# Check if the first bowtie index file has been made; if not, build it\n",
    "if [ ! -f ${REFPREFIX}.1.bt2 ]; then\n",
    "    bowtie2-build ${REFPREFIX}.fasta ${REFPREFIX}\n",
    "fi\n",
    "\n",
    "bowtie2 --rg-id ${INDEX} --rg SM:${INDEX} -X 1000 -x ${REFPREFIX} \\\n",
    "\t-1 ${R1FILE} \\\n",
    "\t-2 ${R2FILE} \\\n",
    "    -U \"${R1FILE/R1P/R1U},${R2FILE/R2P/R2U}\" \\\n",
    "\t| samtools view -hbS -o ${BAMDIR}/${INDEX}.unsorted.bam\n",
    "\n",
    "samtools sort -m 10000000 -o ${BAMDIR}/${INDEX}.bam -O bam -T ${BAMDIR}/${INDEX} ${BAMDIR}/${INDEX}.unsorted.bam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bam_depth_table.sh & submit_depth_table.sh**\n",
    "* Description:  \n",
    "  Record the coverage of each sample at each base pair to be used later for filtering and QC.\n",
    "* Newest versions: bam_depth_table.sh & submit_depth_table_070819.sh\n",
    "* Input:\n",
    "    * BAMDIR, (PATH) where the BAM files are stored\n",
    "    * REF, (PATH) reference genome full fasta\n",
    "    * OUTDIR, (PATH) where to put the completed coverage data\n",
    "    * LOG, (PATH) where to save error/output messages\n",
    "* Output: Prints \"Submitting [SAMPLE NAME]\" for each sample as it is queued in the cluster\n",
    "* Usage Notes:\n",
    "    * FASTQ and BAM directories should be different folders located in the same parent directory.\n",
    "    * Change the `-f10` parameter as needed by your directory structure.\n",
    "    * A 2-hour job time is generous. 30 min to 1 hr should be plenty.\n",
    "    * Run the submission script from terminal/console.\n",
    "* Submission and main bash scripts from 05/24/19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Submit alignment files for coverage tabulation (before marking dup's)\n",
    "# Where bam files are stored. Will only use .dm.bam files.\n",
    "BAMDIR=/oasis/tscc/scratch/sguy/SafeGenes/data/mut_accumulation/bam\n",
    "# Reference sequence so we can include all zero depth / unread seq's\n",
    "export REF=/oasis/tscc/scratch/sguy/SafeGenes/data/reference_genomes/Sc_W303_v2_yKC27_genomic.fasta\n",
    "# Output/error files from TORQUE\n",
    "LOG=/oasis/tscc/scratch/sguy/SafeGenes/log/dp_table\n",
    "# Name your output files\n",
    "OUTDIR=/oasis/tscc/scratch/sguy/SafeGenes/data/mut_accumulation/depth\n",
    "\n",
    "for bamfile in $BAMDIR/*.dm.bam; do\n",
    "    export INBAM=$bamfile\n",
    "    sample=\"$( \\\n",
    "        echo ${INBAM} | \\\n",
    "        cut -d '/' -f 10 | \\\n",
    "        cut -d '.' -f 1 \\\n",
    "        )\"\n",
    "    export OUTPUT=$OUTDIR/$sample.depth\n",
    "    echo \"Submitting $sample\"\n",
    "    qsub \\\n",
    "        -V \\\n",
    "        -N dp_$sample \\\n",
    "        -o ${LOG}_${sample}.out \\\n",
    "        -e ${LOG}_${sample}.err \\\n",
    "        /oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/bam_depth_table.sh\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l nodes=1,walltime=02:00:00\n",
    "#PBS -M sguy@ucsd.edu\n",
    "#PBS -m abe\n",
    "\n",
    "# Given a reference and BAM alignment, table all read depths\n",
    "\n",
    "#Start by printing input to error file for easier tracing\n",
    "echo Reference file: $REF; echo BAM in: $INBAM; echo Table Out: $OUTPUT\n",
    "\n",
    "module load samtools\n",
    "samtools depth -aa --reference $REF $INBAM > $OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**picard_sam.sh & submit_picard.sh**\n",
    "* Description:  \n",
    "  Label identical reads, which are likely just artifacts of PCR during library prep. Reads marked as identical will be grouped as a single read during variant calling. Generates new BAM files with extension \".dm.bam\".\n",
    "* Newest version: 052419\n",
    "* Input:\n",
    "  * BAMDIR, (PATH) directory with BAM files\n",
    "  * REFPREFIX, (PATH) reference genome file, excluding any extensions\n",
    "  * GATK, (PATH) which version of GATK to use\n",
    "* Output: Prints \"Submitting [SAMPLE NAME]\" for each sample as it is queued in the cluster\n",
    "* Usage notes:\n",
    "  * You should make a \"temp_dir\" in the BAM directory. This will store alignment summaries that are used further down.\n",
    "  * Change the `-f10` modifier as needed\n",
    "  * The most recent version will skip any files that already have a duplicate-marked BAM file. If you would like to generate new duplicate-marked files, delete or move the old .dm.bam file to a different directory.\n",
    "  * GATK can be installed anywhere as long as the TSCC terminals can access that path. Your scratch directory works.\n",
    "  * Run the submission script from terminal/console.\n",
    "  * CAUTION: Ensure that all alignment files are present at the end. Count the total number of files in a directory using `ls -l | wc -l`. GATK or Java may occasionally crash, ending the job but not always throwing up an error to TSCC. If alignments are missing, run the submission script again without changes, as it is coded to skip an files for which an alignment already exists.\n",
    "* Submission and main scripts from 05/24/19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Mark duplicate reads in BAM files with Picard tools\n",
    "\n",
    "LOG=/oasis/tscc/scratch/sguy/SafeGenes/log\n",
    "export DATADIR=/oasis/tscc/scratch/sguy/SafeGenes/data/mut_accumulation/bam\n",
    "export BAMDIR=$DATADIR\n",
    "export REFPREFIX=/oasis/tscc/scratch/sguy/SafeGenes/data/reference_genomes/Sc_W303_v2_yKC27_genomic\n",
    "export GATK=/oasis/tscc/scratch/sguy/SafeGenes/software/gatk-4.0.9.0/gatk\n",
    "\n",
    "# Submitting jobs in a loop for files that weren't made\n",
    "for bamfile in ${BAMDIR}/*.bam; do\n",
    "    export BAM=${bamfile}\n",
    "    name_only=\"$(cut -d'/' -f10 <<<${bamfile})\"\n",
    "    export INDEX=${name_only/.bam/}\n",
    "    if [ ! -f ${bamfile/.bam/.dm.bam} ]; then\n",
    "        echo \"Submitting ${INDEX}...\"\n",
    "        qsub \\\n",
    "            -N picard-${INDEX} \\\n",
    "            -V \\\n",
    "            -o ${LOG}/picard-${INDEX}_052419.out \\\n",
    "            -e ${LOG}/picard-${INDEX}_052419.err \\\n",
    "            /oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/picard_sam_052419.sh\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l nodes=1,walltime=1:00:00\n",
    "#PBS -M sguy@ucsd.edu\n",
    "#PBS -m abe\n",
    "\n",
    "#Switching to Java 1.8 for GATK v4.X to function correctly.\n",
    "export PATH=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/bin:$PATH\n",
    "module load samtools\n",
    "\n",
    "# Create a reference genome index w/ unique ID to avoid clashes\n",
    "cp ${REFPREFIX}.fasta ${REFPREFIX}_${INDEX}.fasta\n",
    "samtools faidx ${REFPREFIX}_${INDEX}.fasta\n",
    "\n",
    "# Create summary statistics for read alignments\n",
    "${GATK} --java-options \"-Xmx1500M\" CollectAlignmentSummaryMetrics \\\n",
    "\t--INPUT=${BAM} \\\n",
    "\t--OUTPUT=${DATADIR}/picard_metrics/alignment_summary-${INDEX}.txt \\\n",
    "\t--REFERENCE_SEQUENCE=${REFPREFIX}_${INDEX}.fasta\n",
    "\n",
    "# Mark duplicate reads\n",
    "${GATK} --java-options \"-Xmx1500M\" MarkDuplicates \\\n",
    "\t--INPUT=${BAM} \\\n",
    "\t--OUTPUT=${BAM/.bam/.dm.bam} \\\n",
    "\t--METRICS_FILE=${DATADIR}/picard_metrics/picard-${INDEX}.txt \\\n",
    "\t--ASSUME_SORTED=TRUE \\\n",
    "    --TMP_DIR=${DATADIR}/temp_dir\n",
    "\n",
    "samtools index ${BAM/.bam/.dm.bam}\n",
    "\n",
    "rm ${REFPREFIX}_${INDEX}.fasta\n",
    "rm ${REFPREFIX}_${INDEX}.fasta.fai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bam_depth_table.sh & submit_depth_table.sh**\n",
    "* Description:  \n",
    "  Generate tables that provide the coverage at each base pair along the genome of each DNA sample. This information assists variant filter further down the pipeline.\n",
    "* Newest version: 042119 (submission script)\n",
    "* Input:\n",
    "  * BAM, (PATH) location of BAM files with duplicate reads marked\n",
    "  * REF, (PATH) reference genome of your particular strain\n",
    "  * LOG, (PATH) where to save TSCC error/output text files\n",
    "  * OUTDIR, (PATH) where to save the tables of coverage data\n",
    "* Output:\n",
    "  * Prints \"Submitting [SAMPLE NAME]\" as it submits jobs to the queue\n",
    "  * Generates one plain-text, tab-delimited table per sample containing coverage at each base pair along the genome\n",
    "* Usage notes:\n",
    "  * Run this after duplicates have been marked.\n",
    "  * In the example below, the script has been written in triplicate. This was due to laziness (the author did not feel like constructing a complicated for-loop in bash to accomodate three strains with different reference genomes). The user may remove repeated code and run the analysis on a single set of BAM files at one time.\n",
    "  * Do customize the `qsub` and `#PBS` commands in order to organize error and output handling by the cluster.\n",
    "* Submission and main scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Submit alignment files for coverage tabulation (before marking dup's)\n",
    "# Where bam files are stored. Will only use .dm.bam files.\n",
    "BAM1=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC27/bam\n",
    "BAM2=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC28/bam\n",
    "BAM3=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC29/bam\n",
    "# Reference sequence so we can include all zero depth / unread seq's\n",
    "export REF1=/oasis/tscc/scratch/sguy/SafeGenes/data/reference_genomes/Sc_W303_v2_yKC27_genomic.fasta\n",
    "export REF2=/oasis/tscc/scratch/sguy/SafeGenes/data/reference_genomes/Sc_W303_v2_yKC28_genomic.fasta\n",
    "export REF3=/oasis/tscc/scratch/sguy/SafeGenes/data/reference_genomes/Sc_W303_v2_yKC29_genomic.fasta\n",
    "# Output/error files from TORQUE\n",
    "LOG=/oasis/tscc/scratch/sguy/SafeGenes/log/dp_table\n",
    "# Name your output files\n",
    "OUTDIR=/oasis/tscc/scratch/sguy/SafeGenes/data/all_UG_vcf/depth\n",
    "\n",
    "# Select strain reference\n",
    "export REF=$REF1\n",
    "for bamfile in $BAM1/*.dm.bam; do\n",
    "    export INBAM=$bamfile\n",
    "    sample=\"$( \\\n",
    "        echo ${INBAM} | \\\n",
    "        cut -d '/' -f 10 | \\\n",
    "        cut -d '.' -f 1 \\\n",
    "        )\"\n",
    "    export OUTPUT=$OUTDIR/$sample.depth\n",
    "    echo \"Submitting $sample\"\n",
    "    qsub \\\n",
    "        -V \\\n",
    "        -N dp_$sample \\\n",
    "        -o ${LOG}_${sample}.out \\\n",
    "        -e ${LOG}_${sample}.err \\\n",
    "        /oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/bam_depth_table.sh\n",
    "done\n",
    "\n",
    "# Select strain reference\n",
    "export REF=$REF2\n",
    "for bamfile in $BAM2/*.dm.bam; do\n",
    "    export INBAM=$bamfile\n",
    "    sample=\"$( \\\n",
    "        echo ${INBAM} | \\\n",
    "        cut -d '/' -f 10 | \\\n",
    "        cut -d '.' -f 1 \\\n",
    "        )\"\n",
    "    export OUTPUT=$OUTDIR/$sample.depth\n",
    "    echo \"Submitting $sample\"\n",
    "    qsub \\\n",
    "        -V \\\n",
    "        -N dp_$sample \\\n",
    "        -o ${LOG}_${sample}.out \\\n",
    "        -e ${LOG}_${sample}.err \\\n",
    "        /oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/bam_depth_table.sh\n",
    "done\n",
    "\n",
    "# Select strain reference\n",
    "export REF=$REF3\n",
    "for bamfile in $BAM3/*.dm.bam; do\n",
    "    export INBAM=$bamfile\n",
    "    sample=\"$( \\\n",
    "        echo ${INBAM} | \\\n",
    "        cut -d '/' -f 10 | \\\n",
    "        cut -d '.' -f 1 \\\n",
    "        )\"\n",
    "    export OUTPUT=$OUTDIR/$sample.depth\n",
    "    echo \"Submitting $sample\"\n",
    "    qsub \\\n",
    "        -V \\\n",
    "        -N dp_$sample \\\n",
    "        -o ${LOG}_${sample}.out \\\n",
    "        -e ${LOG}_${sample}.err \\\n",
    "        /oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/bam_depth_table.sh\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l nodes=1,walltime=02:00:00\n",
    "#PBS -M sguy@ucsd.edu\n",
    "#PBS -m abe\n",
    "\n",
    "# Given a reference and BAM alignment, table all read depths\n",
    "\n",
    "#Start by printing input to error file for easier tracing\n",
    "echo Reference file: $REF; echo BAM in: $INBAM; echo Table Out: $OUTPUT\n",
    "\n",
    "module load samtools\n",
    "samtools depth -aa --reference $REF $INBAM > $OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coverage_slide.py, run_cov_slide.sh, & qsub_cov_slide.sh**\n",
    "* Description:  \n",
    "  Average coverage over a sliding window across each genome. This is useful because deletion mutations inherently have 0 coverage, so measuring coverage around each locus should provide a better measure of confidence.\n",
    "* Newest versions: coverage_slide.v0.1.py & qsub_cov_slide_052419.sh\n",
    "* Input:\n",
    "  * CSLIDE, (PATH) path to the sliding coverage python script\n",
    "  * DPDIR, (PATH) location of depth tables generated by `samtools depth`\n",
    "  * LOG, (PATH) where to save TSCC error/output text files\n",
    "  * RUN, (PATH) script that sets up the environment and runs the python script\n",
    "* Output:\n",
    "  * Prints each file name as it is submitted to queue\n",
    "  * Generates one tab-delimited table per sample that provides the averaged coverage at each base pair along the genome\n",
    "* Usage Notes:\n",
    "  * Remember to edit the `qsub` options to send messages to your directories and emails.\n",
    "  * The submission script uses `\"$(echo $NAME | cut -d'/' -f2)\"` to identify the sample name. Adjust the `-f2` flag according to how your directories are organized.\n",
    "  * Change `wid=100` in run_cov_slide.sh to adjust the size of the windows in base pairs. The window width is centered on each locus when averaging coverage, so loci on the ends are an average coverage across no less than half the width of the window.\n",
    "  * To clarify, qsub_cov_slide.sh submits a job to run on TSCC, and run_cov_slide.sh runs the python script when the job is completed.\n",
    "  * This step was not in place during analysis of the Asexual Evolution sequencing data. These scripts were only written once analysis on the preliminary data set of Mutation Accumulation lines had begun. For all future uses, this step should be incorporated into the pipeline after coverage tables are generated, but be wary of potential data type inconsistencies for Python.\n",
    "  * It may be difficult to run the python script on TSCC because pandas ([documentation](https://pandas.pydata.org/pandas-docs/stable/)) was not readily available on their default environment. It is possible to install your own distribution of Anaconda2 in your scratch directory in order to more easily install and run your own packages ([instructions for pandas](https://pandas.pydata.org/pandas-docs/stable/install.html)). Using `module load scipy` should load a version of python with most needed packages, but we've struggled to get this solution to work.\n",
    "  * Alternatively, one may opt to import the coverage tables from their scratch directory and process them locally, since the python script is not very memory- or CPU-intensive.\n",
    "* Submission script, bash handling script, and main python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Queue up .depth files for sliding window coverage analysis\n",
    "\n",
    "# Main python script\n",
    "export CSLIDE=/oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/coverage_slide.v0.1.py\n",
    "# Depth table directory\n",
    "export DPDIR=/oasis/tscc/scratch/sguy/SafeGenes/data/mut_accumulation/depth\n",
    "# Log directory for torque\n",
    "LOG=/oasis/tscc/scratch/sguy/SafeGenes/log\n",
    "# Bash script to run the python script\n",
    "RUN=/oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/run_cov_slide.sh\n",
    "\n",
    "# Submit each table as a separate job\n",
    "for dtable in ${DPDIR}/*.depth; do\n",
    "    export INTABLE=$dtable\n",
    "    export OUTABLE=${dtable/.depth/.sw.depth}\n",
    "    NAME=${dtable/$DPDIR/}\n",
    "    NAME=\"$(echo $NAME | cut -d'/' -f2)\"\n",
    "    qsub \\\n",
    "        -V \\\n",
    "        -l walltime=00:30:00 \\\n",
    "        -M sguy@ucsd.edu \\\n",
    "        -m abe \\\n",
    "        -N $NAME \\\n",
    "        -o ${LOG}/coverage_slide_${NAME}.out \\\n",
    "        -e ${LOG}/coverage_slide_${NAME}.err \\\n",
    "        $RUN\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Run the sliding window coverage analysis program\n",
    "/oasis/tscc/scratch/sguy/anaconda2/bin/python ${CSLIDE} src=${INTABLE} out=${OUTABLE} wid=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Perform a sliding window coverage conversion on output from samtools\n",
    "depth. This reduces bias against deletions if when filtering variant\n",
    "calls. Will crawl by single-bp along each contig/chromosome and take\n",
    "the average depth of a window around the position. This window is\n",
    "cropped when it encounters the boundary of a contig.\n",
    "\n",
    "Example\n",
    "$ python ./coverage_slide.v0.1.py src=input.dp out=out.dp wid=100\n",
    "\n",
    "Parameters\n",
    ":@param src: (str)\n",
    "             Path to input from samtools depth\n",
    ":@param out: (str)\n",
    "             Path to save sliding window output\n",
    ":@param wid: (wid > 0)\n",
    "             Width of the sliding window centered around target bp\n",
    "'''\n",
    "\n",
    "# Table and math packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# IO / file handling\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Create dictionary to store key input variables\n",
    "input_vars = {}\n",
    "# Call default input, output path\n",
    "input_vars[\"src=\"] = \"./sample.depth\"\n",
    "input_vars[\"out=\"] = \"./sample.sw.depth\"\n",
    "# Call default window width\n",
    "input_vars[\"wid=\"] = 100\n",
    "\n",
    "# Check user input\n",
    "for given in sys.argv:\n",
    "    # Check if input has a flag\n",
    "    if given[:4] in input_vars:\n",
    "        # Re-assign variables when flag detected\n",
    "        input_vars[given[:4]] = given[4:]\n",
    "    # Do not use unflagged inputs\n",
    "    else:\n",
    "        print \"Not used for input/output:\", given\n",
    "# Check that the input file path is right\n",
    "assert( os.path.isfile(input_vars[\"src=\"]) ), \\\n",
    "    \"No file found at input: {0}\".format(input_vars[\"src=\"])\n",
    "# Check that the width is integer\n",
    "try:\n",
    "    int(input_vars[\"wid=\"])\n",
    "except TypeError:\n",
    "    print input_vars[\"wid=\"], \"is not an integer\"\n",
    "# Convert width from a string input into an integer\n",
    "input_vars[\"wid=\"] = int(input_vars[\"wid=\"])\n",
    "# Check that the width is positive\n",
    "assert( input_vars[\"wid=\"] > 0 ), \\\n",
    "    \"Width must be a positive integer\"\n",
    "\n",
    "# Report progress\n",
    "print \"Loading source depth table:\", input_vars[\"src=\"]\n",
    "# Open the source depth file as a DataFrame\n",
    "src_depth = pd.read_csv( input_vars[\"src=\"],\n",
    "    header=None, sep=\"\\t\"\n",
    ")\n",
    "# Assign column headers to make things easier to follow\n",
    "src_depth.columns = [\"chrom\", \"pos\", \"cov\"]\n",
    "\n",
    "# Perform analysis by chromosome\n",
    "for chrom, chr_cov in src_depth.groupby(\"chrom\"):\n",
    "    # Report progress\n",
    "    print \"Analyzing {0}\".format(chrom)\n",
    "    # Generate a sliding window mean\n",
    "    window_means = chr_cov.rolling(\n",
    "        # PATCH: Look up different window options later\n",
    "        input_vars[\"wid=\"], center=True, min_periods=1\n",
    "    ).mean()\n",
    "    # Apply sliding window data to the source\n",
    "    src_depth.loc[ src_depth[\"chrom\"] == chrom, \"sliding_mean\" ] \\\n",
    "        = window_means[\"cov\"]\n",
    "\n",
    "# Report progress\n",
    "print \"Exporting sliding window table:\", input_vars[\"out=\"]\n",
    "# Export the modified source depth table\n",
    "src_depth.to_csv( input_vars[\"out=\"], sep=\"\\t\", index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**submit_gatk.sh & run_GATK.sh**\n",
    "* Description:  \n",
    "  Call variants using the formatted BAM alignment files. Output new VCF's to the desired directory.\n",
    "* Newest version: 041819\n",
    "* Input:\n",
    "  * DATADIR, (PATH) parent directory of the BAM and VCF directories\n",
    "  * REFPREFIX, (PATH) path to genome files excluding file extension\n",
    "  * GATK, (PATH) where the GATK package is located\n",
    "  * JAVA, (PATH) which version of Java to use\n",
    "  * OUTDIR, (PATH) where to save the resulting VCF files\n",
    "  * BAMDIR, (PATH) location of the duplicate-marked, BAM alignment files\n",
    "* Output:\n",
    "  * Prints \"Submitting [SAMPLE NAME] for HaplotypeCaller\" for each sample as it is queued in the cluster\n",
    "  * Generates one VCF per sample submitted to the sequencer.\n",
    "* Usage Notes:\n",
    "  * After much research and debate, we decided to use the legacy UnifiedGenotyper variant calling function instead of HaplotypeCaller, which is newer and still supported with updates. Despite many attempts to correct its behavior, HaplotypeCaller appeared to be sub-sampling our reads, reducing our confidence in the haplotype calls that it made (see Asexual Yeast Evo Log SEG on April 18, 2019).\n",
    "  * Because UnifiedGenotyper is an older piece of software, it outputs an VCF files in an older format. This impacts how multi-allelic loci and other variant tags are handled later in the pipeline. Details on the current format can be found on the [Samtools website](http://samtools.github.io/hts-specs/).\n",
    "  * Note the switch to GATK 2.6 and JAVA JDK 7 in order to use Unified Genotyper. The TSCC has its own set of Java environments to pick from, so use those instead of anything in a scratch directory.\n",
    "  * The console output from the submission script says HaplotypeCaller, but it really is UnifiedGenotyper. This should be patched next time it is used.\n",
    "  * Run the submission script directly from console.\n",
    "* Submission and main scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# This script uses the GATK to call variants\n",
    "\n",
    "export REFPREFIX=/oasis/tscc/scratch/sguy/SafeGenes/data/reference_genomes/Sc_W303_v2_yKC27_genomic\n",
    "export DATADIR=/oasis/tscc/scratch/sguy/SafeGenes/data/yKC27\n",
    "export GATK=/oasis/tscc/scratch/sguy/SafeGenes/software/GenomeAnalysisTK-2.6-5-gba531bd/GenomeAnalysisTK.jar\n",
    "export JAVA=/oasis/tscc/scratch/sguy/SafeGenes/software/jdk1.7.0_80/bin/java\n",
    "export OUTDIR=${DATADIR}/vcf_UG\n",
    "export BAMDIR=${DATADIR}/bam\n",
    "\n",
    "# Adjust cut -f option to directory tree; grab pop-gen from file name\n",
    "for gzbam in ${BAMDIR}/*.dm.bam; do\n",
    "    export BAMFILE=$gzbam\n",
    "    export INDEX=\"$( \\\n",
    "        echo ${BAMFILE} | \\\n",
    "        cut -d '/' -f 10 | \\\n",
    "        cut -d '.' -f 1 \\\n",
    "        )\"\n",
    "    export OUTPUT=\"${OUTDIR}/${INDEX}_variant_calls.vcf\"\n",
    "    echo \"Submitting ${INDEX} for GATK HaplotypeCaller\"\n",
    "    qsub -N \"GATK-${INDEX}\" \\\n",
    "        -V \\\n",
    "        -o /oasis/tscc/scratch/sguy/SafeGenes/log/gatk_${INDEX}_041819.out \\\n",
    "        -e /oasis/tscc/scratch/sguy/SafeGenes/log/gatk_${INDEX}_041819.err \\\n",
    "        /oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/run_GATK_041819.sh\n",
    "    done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l walltime=10:00:00\n",
    "#PBS -M sguy@ucsd.edu\n",
    "#PBS -m abe\n",
    "\n",
    "${JAVA} -jar -Xmx2g ${GATK} \\\n",
    "\t-T UnifiedGenotyper \\\n",
    "\t-R ${REFPREFIX}.fasta \\\n",
    "\t-I ${BAMFILE} \\\n",
    "\t--genotype_likelihoods_model BOTH \\\n",
    "\t--max_alternate_alleles 8 \\\n",
    "\t-stand_call_conf 4 \\\n",
    "\t-o ${OUTPUT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**combine_vcfs.sh & malformed_vcf.py**\n",
    "* Description:  \n",
    "  Reformats and compiles VCF files from all samples onto a single table to simplify downstream filtering and analysis. On the side, will generate a single VCF file containing all variant information from constituent VCFs.\n",
    "* Newest Version: combine_vcfs_042019.sh & malformed_vcf.v1.1.py\n",
    "* Input:\n",
    "  * VCFDIR, (PATH) where VCF files are located\n",
    "  * FULLVCF, (PATH) file name of the fully compiled VCF file\n",
    "  * OUTPUT, (PATH) file name of the tabulated version of the compiled VCF file\n",
    "  * BGZIP, (PATH) Samtools software for compressing and decompressing HTS format files\n",
    "  * JAVA, (PATH) Java version compatible with GATK (depends VCF version)\n",
    "  * GATK, (PATH) GATK package used for final tabulartion of compiled VCF\n",
    "  * FIXVCF, (PATH) script for reporting and fixing format errors in the compiled VCF\n",
    "  * REF, (PATH) reference genome used for all variant calls\n",
    "* Output:\n",
    "  * Compresses all individual VCF files\n",
    "  * Creates index files for each VCF files\n",
    "  * Compiles all VCF files into a master VCF and converts it to a simpler, tab-delimited format\n",
    "  * Reports how many variants were lost due to formatting issues\n",
    "* Usage Notes:\n",
    "  * Version 042019 is set up to work with the VCF3 output of UnifiedGenotyper. Formating issues will arise if VCF4 files are used as input.\n",
    "  * In the example below, some lines are commented out. This is because the script was run multiple times in the process of troubleshooting, and repeating some of the steps would have caused an error and terminated job. Remove such comment `#` tags before running this script for the first time.\n",
    "  * As always, adjust the submission flags for output/error messages.\n",
    "  * The python script serves to identify and remedy formatting errors prior to generating the final variant table.\n",
    "  * Submit the bash script directly to queue: `qsub ./combine_vcfs.sh`.\n",
    "* Full tabulation and formatting scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l walltime=01:00:00\n",
    "#PBS -M sguy@ucsd.edu\n",
    "#PBS -m abe\n",
    "#PBS -N combine_vcf\n",
    "#PBS -o /oasis/tscc/scratch/sguy/SafeGenes/log/combine_vcf_042019.out\n",
    "#PBS -e /oasis/tscc/scratch/sguy/SafeGenes/log/combine_vcf_042019.err\n",
    "\n",
    "# Switching to Java 1.8\n",
    "export PATH=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.31-1.b13.el6_6.x86_64/bin:$PATH\n",
    "\n",
    "# Script for merging VCFs from multiple samples\n",
    "module load bcftools\n",
    "module load samtools\n",
    "module load python\n",
    "\n",
    "# Make sure directory only contains your desired files\n",
    "VCFDIR=/oasis/tscc/scratch/sguy/SafeGenes/data/all_UG_vcf\n",
    "FULLVCF=/oasis/tscc/scratch/sguy/SafeGenes/data/all_UG_vcf/presplit_calls_042019.vcf.gz\n",
    "OUTPUT=${FULLVCF/.gz/.txt}\n",
    "BGZIP=/opt/biotools/trinity/trinity-plugins/htslib/bgzip\n",
    "JAVA=/oasis/tscc/scratch/sguy/SafeGenes/software/jdk1.7.0_80/bin/java\n",
    "GATK=/oasis/tscc/scratch/sguy/SafeGenes/software/GenomeAnalysisTK-2.6-5-gba531bd/GenomeAnalysisTK.jar\n",
    "FIXVCF=/oasis/tscc/scratch/sguy/SafeGenes/code/rice_pipeline/src/malformed_vcf.v1.1.py\n",
    "REF=/oasis/tscc/scratch/sguy/SafeGenes/data/reference_genomes/Sc_W303_v2_yKC27_genomic.fasta\n",
    "\n",
    "# Zipping all files, keeping indices intact\n",
    "# for vcfile in ${VCFDIR}/*.vcf; do ${BGZIP} $vcfile; done\n",
    "\n",
    "# Saving list of the newly zipped files while indexing with tabix\n",
    "# VCFZIP=$(ls ${VCFDIR}/*.vcf.gz)\n",
    "VCFZIP=\n",
    "for vcffile in ${VCFDIR}/*.vcf.gz; do\n",
    "    tabix -p vcf $vcffile\n",
    "    VCFZIP=\"${VCFZIP}-V ${vcffile} \"\n",
    "done\n",
    "\n",
    "# Merging files horizontally, across samples, into a zipped VCF\n",
    "#bcftools merge -o ${FULLVCF} -Oz ${VCFZIP}\n",
    "$JAVA -jar -Xmx2G $GATK -T CombineVariants $VCFZIP -R $REF -o $FULLVCF\n",
    "# Unzip the file so it's easier to parse\n",
    "$BGZIP -d $FULLVCF\n",
    "# Clear out formatting issues w/ * from the merge\n",
    "python ${FIXVCF} ${FULLVCF/.gz/} ${FULLVCF/.gz/}\n",
    "# Compress it again\n",
    "$BGZIP ${FULLVCF/.gz/}\n",
    "\n",
    "# Creating .tbi index file for table conversion\n",
    "# $JAVA -jar -Xmx2G $GATK -T IndexFeatureFile -F ${FULLVCF}\n",
    "tabix -p vcf $FULLVCF\n",
    "\n",
    "# Running the data pull and printing run time; organizing cols by input source\n",
    "$JAVA -jar -Xmx2G $GATK -T VariantsToTable \\\n",
    "    -V ${FULLVCF} \\\n",
    "    -R $REF \\\n",
    "    -SMA \\\n",
    "    -AMD \\\n",
    "    -F CHROM -F POS -F REF -F ALT \\\n",
    "    -GF AD -GF DP \\\n",
    "    -o ${OUTPUT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fix malformed VCF files. Will focus on specific patterns. Use in bash.\n",
    "\n",
    "User$ python malformed_vcf.v1.0.py [VCF] [OUT]\n",
    "\n",
    ":param vcf: (str) path to the VCF file to be fixed\n",
    ":param OUT: (str) path to which the new file will be made\n",
    ":out : None, will just edit the file that was given\n",
    "'''\n",
    "\n",
    "# Load I/O modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check bash input\n",
    "assert( len(sys.argv) == 3 ), \"Incorrect number of bash inputs\"\n",
    "\n",
    "# Get the VCF file from bash input\n",
    "vcf_path = sys.argv[1]\n",
    "# Get the ouptut path\n",
    "out_path = sys.argv[2]\n",
    "\n",
    "# There should be a vcf file at the end of the path\n",
    "assert( os.path.isfile( vcf_path ) ), \"No file found at \" + vcf_path\n",
    "assert( vcf_path[-4:] == '.vcf' ), \"File is missing .vcf extension\"\n",
    "\n",
    "# Hold the repaired lines in a string to be printed later\n",
    "reformed_vcf = ''\n",
    "# Line counter to empty reformed_vcf into buffer file when full\n",
    "lines_held = 0\n",
    "\n",
    "\n",
    "# Main *ATCG removal algorithm; new -> remove multiple *'s\n",
    "def fix_bad_row( inrow ):\n",
    "    '''\n",
    "    Takes in a line and removes any * next to a A, T, C, or G, since\n",
    "    the notations would be equivalent and indexing tends to glitch when\n",
    "    these character combinations occur.\n",
    "\n",
    "    Note: This may not work correctly if * is the first or last letter\n",
    "    in the line, but this should happen rarely if the input comes from\n",
    "    an actual VCF.\n",
    "\n",
    "    Patterns to remove:\n",
    "    *A, *T, *C, *G, A*, T*, C*, G*\n",
    "\n",
    "    :param inrow: (str) Row from a VCF file that has a *\n",
    "    :output: Same row w/o the asterisk error \n",
    "    '''\n",
    "    # Start building string for the output\n",
    "    outrow = ''\n",
    "\n",
    "    # Loop through each char from input\n",
    "    for i, letr in enumerate(inrow):\n",
    "        # Add anything not an asterisk\n",
    "        if letr != '*':\n",
    "            outrow += letr\n",
    "        # Check chara to left of *'s\n",
    "        elif letr == '*' and inrow[i-1] in 'ATCG':\n",
    "            # Report edit to log\n",
    "            print 'Position', i, 'removed from', inrow\n",
    "            # Skip *'s location if ATCG on left\n",
    "            continue\n",
    "        # Check chara to right of *'s\n",
    "        elif letr == '*' and inrow[i+1] in 'ATCG':\n",
    "            # Report edit to log\n",
    "            print 'Position', i, 'removed from', inrow\n",
    "            # Skip *'s location if ATCG on left\n",
    "            continue\n",
    "        # Just add *'s not next to ATCG\n",
    "        else:\n",
    "            outrow += letr\n",
    "    \n",
    "    # Use the processed VCF row\n",
    "    return outrow\n",
    "\n",
    "\n",
    "# Open up the VCF\n",
    "with open( vcf_path, 'r' ) as source_vcf:\n",
    "    # Go thru each line\n",
    "    for line in source_vcf:\n",
    "        # Just copy header & lines w/o *\n",
    "        if line[0] == '#' or '*' not in line:\n",
    "            reformed_vcf += line\n",
    "        # When an '*' is present\n",
    "        else:\n",
    "            # Run thru correction function and add to string\n",
    "            reformed_vcf += fix_bad_row( line )\n",
    "        # After line is processed, add to counter\n",
    "        lines_held += 1\n",
    "\n",
    "# Show result\n",
    "print lines_held, \"lines filtered from\", vcf_path\n",
    "# Overwrite original file\n",
    "\n",
    "# Create a file at the output path\n",
    "with open( out_path, 'w+' ) as new_vcf:\n",
    "    # Write the filtered VCF to the output\n",
    "    new_vcf.write(reformed_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add_depths2vcf.py**\n",
    "* Description:\n",
    "  Adds the actual coverage from the alignment data to the master variant table such that variants can later be filtered by coverage.\n",
    "* Newest Version: v0.2\n",
    "* Input:\n",
    "  * tab, (PATH) the master variant table\n",
    "  * depths, (PATH) directory containing the coverage tables\n",
    "  * tabout, (PATH) where to save the table with the added coverage data\n",
    "  * dp_ext, (str, OPTIONAL) the file extension of the coverage tables; default \".depth\"\n",
    "* Output:\n",
    "  * New tab-delimited variant table with coverage taken directly from the BAM alignment files\n",
    "  * Prints a confirmation of where the new table was saved\n",
    "* Usage Notes:\n",
    "  * The user may import the master variant table and coverage tables to their personal computer or to Google Drive and do this step locally. If they would like to submit this to TSCC as a job, they would have to write their own submission script and make sure that pandas and numpy are available.\n",
    "  * This script is designed to work from bash or command line. Check the documentation at the top of the script for formatting the input.\n",
    "  * Do not use escape characters in any of the path variables. Just put the file or directory path in double-quotes.\n",
    "  * This script overwrites the original coverage data. Thus, it is not suggested to overwrite the original master variant table, so save it to a different file name or directory.\n",
    "  * This script is compatible with output from coverage_slide.v0.1.py. Use these coverage measurements such that coverage of deletions depends on their context rather than their absence of any reads.\n",
    "* Full script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Overview:\n",
    "Add read depths from samtools depth results to a VCF table generated by\n",
    "GATK VariantsToTable. This helps in determine whether a variant call\n",
    "failed due to a lack of non-reference reads or poor coverage.\n",
    "\n",
    "Usage:\n",
    "Intended for use from a bash script or terminal. Flags may be given in\n",
    "any order. If the specified output file already exists, will overwrite\n",
    "the existing file.\n",
    "\n",
    "python <path>/add_depth2vcf.v0.1.py \\\\\n",
    "    tab=<input> \\\\\n",
    "    depths=<input> \\\\\n",
    "    tabout=<input> \\\\\n",
    "    [dp_ext=.depth]\n",
    "\n",
    "New in Version 0.2:\n",
    "-Compatible only with output from coverage_slide.v0.1.py\n",
    "\n",
    ":@param tab: (str) Path to variant table from VariantsToTable\n",
    ":@param depths: (str) Path to dir with depth tables from samtools depth\n",
    ":@param dp_ext: (str) File extension of depth tables; default .depth\n",
    ":@param tabout: (str) Names output table; default ./my_vcf_depth.txt\n",
    "\n",
    "'''\n",
    "\n",
    "#Modules for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Modules for accessing files/directories on the system\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#Call input variables\n",
    "tab = ''\n",
    "depths = ''\n",
    "depth_ext = '.depth'\n",
    "tabout = './my_vcf_depth.txt'\n",
    "\n",
    "#Record input from bash\n",
    "for bashin in sys.argv:\n",
    "    #Check for tab file input flag\n",
    "    if bashin[:4] == 'tab=':\n",
    "        tab = bashin[4:]\n",
    "        #Check if tab input leads to a file\n",
    "        assert( os.path.isfile(tab) ), \\\n",
    "            '{0} is not a file'.format(tab)\n",
    "    #Check depth directory input flag\n",
    "    elif bashin[:7] == 'depths=':\n",
    "        depths = bashin[7:]\n",
    "        #Check if tab input leads to a directory\n",
    "        assert( os.path.isdir(depths) ), \\\n",
    "            '{0} is not a dir'.format(depths)\n",
    "    #Check flag for name of the output table\n",
    "    elif bashin[:7] == 'tabout=':\n",
    "        tabout = bashin[7:]\n",
    "    # Check flag for an alternate file extension\n",
    "    elif bashin[:7] == 'dp_ext=':\n",
    "        depth_ext = bashin[7:]\n",
    "    else:\n",
    "        #Skip anything without formatted flag\n",
    "        print \"{0} not used as variable\\n\".format(bashin)\n",
    "\n",
    "#Replace VCF depth values from depth tables by CHROM, POS\n",
    "def add_depth( vtable, dtable ):\n",
    "\n",
    "    '''\n",
    "    Main function for adding the depth values to the VCF table. Note\n",
    "    that depth values from HaplotypeCaller in .DP columns will be\n",
    "    replaced by depths from the alignments themselves. Will tally &\n",
    "    report the number of time that this occurs.\n",
    "\n",
    "    :@param vtable: (pd.DataFrame) VCF table from HaplotypeCaller\n",
    "    :@param dtable: (pd.DataFrame) depth table from samtools depth\n",
    "    :@param newcol: (str) Name new column for sample depth\n",
    "    :@output: (pd.Series) Depth values to replace VCF table DPs\n",
    "    '''\n",
    "    #Change depth table columns to match VCF columns\n",
    "    dtable.columns = ['CHROM', 'POS', 'COVERAGE', 'SLIDING']\n",
    "    # Remove the plain coverage values\n",
    "    dtable = dtable.drop(\"COVERAGE\", axis=1)\n",
    "    #Combine DataFrames by chr, position only on rows present in VCF\n",
    "    newtable = pd.merge(\n",
    "        vtable,\n",
    "        dtable,\n",
    "        how='left',\n",
    "        on=['CHROM', 'POS']\n",
    "        )\n",
    "    #Check that the new dataframe is the correct size\n",
    "    assert( newtable.shape[0] == vtable.shape[0] ), \\\n",
    "        \"Original VCF table has {0} rows.\".format(vtable.shape[0]) \\\n",
    "        + \"Join of VCF table has {1} rows.\".format(newtable.shape[0])\n",
    "    return newtable['SLIDING']\n",
    "\n",
    "#Load tab file as a pandas dataframe; saves correct headers\n",
    "vcf_table = pd.read_table( tab, header=0, low_memory=False )\n",
    "\n",
    "#Loop through depth tables in the depths directory\n",
    "for depth_path in glob.iglob(depths + \"/*\" + depth_ext):\n",
    "    #Get file name from the depth table path\n",
    "    depth_file = depth_path.split( '/' )[-1:][0]\n",
    "    #Get sample name from the file name\n",
    "    sample = depth_file.split( '.' )[0]\n",
    "    #Load depth table as a DataFrame\n",
    "    depth_table = pd.read_table( depth_path, header=0 )\n",
    "    #Check that the sample is in the VCF table\n",
    "    assert( sample + '.DP' in vcf_table.columns ), \\\n",
    "        sample + \".DP not found in VCF table\"\n",
    "    #Add the depth info to the VCF table\n",
    "    vcf_table[sample + \".DP\"] = \\\n",
    "        add_depth( vcf_table, depth_table )\n",
    "    #Report progress\n",
    "    print \"Added\", sample\n",
    "\n",
    "#Save new VCF DataFrame as a text file\n",
    "vcf_table.to_csv( tabout, sep='\\t')\n",
    "\n",
    "#Report finished job\n",
    "print \"New table saved to\", tabout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**next script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}